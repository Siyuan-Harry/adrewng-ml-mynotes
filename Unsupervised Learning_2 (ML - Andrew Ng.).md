# 7 Principal Component Analysis

## 7.1 Reducing the number of features

PCA is commonly used for visualization. Specifically, if you have a dataset with a lot of features, say 10 features, 50 features or even 1000 features, you can't plot 1000 dimensional data.

- PCA, or principal components analysis, can reduce the number of features to **two features**, or **three features**.
- So that you can plot it and visualize it,
- So data scientists can see what is going on.

To exemplify:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616202800.png"/>

- here, $x_1$ feature denotes the **length** of the car, and $x_2$ denotes the **width** of the car. 
  - **Note**: the width of the car is generally constrained a lot because of the width of the road.
- $x_1$ varies a lot but $x_2$ (width) almost show no difference from car to car.
- PCA will determine that, let's  just use $x_1$ feature. 

Another example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616202902.png"/>

- here again, the PCA will decide to just take $x_1$ as a valid feature.

A more complex example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616203528.png"/>

- It seems two features (length && height) are linear correlated. Some cars are bigger, that they **have larger length and larger height at the same time**

- So we construct a **z axis**, which includes both information for $x_1$ and $x_2$ (a linear combination of them both!)

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616203551.png"/>

  - **z axis** can denote the **size** of a car.

- So here is what PCA is doing: find new axis and coordinates. 

One more example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616203749.png"/>

- almost all data points in 3D graphic are living in a very thin surface:

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616203729.png"/>

  - almost like a pancake

- So the PCA can do is, just reduce one dimension (by making combination of dimensions in 3D graphic)

  - "show the pancake in 2D surface"

- The PCA here, creates $z_1$ and $z_2$, two new dimensions.

Next example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616204208.png"/>

- If there are 50 features to measure a country. PCA can compress them into 2 features

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616204312.png"/>

- In the end, you may notice that the two created features $z_1$ and $z_2$ may correspondes to some original features..

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240616204602.png"/>

  - Such as "country's GDP", "per person GDP"

PCA is espicially powerful in visualizing the dataset. And carrying out data visualization is always helpful in letting us understand deeper about the data.

Next:

> let's start to take a look at how exactly the PCA algorithm works.

## 7.2 PCA Algorithm

How to use one feature, that is more effective, to represent the data originally have 2 features?

- Here we have $x_1$ and $x_2$, and there is no label $y$ 

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618120525.png"/>

- You have to **preprocess the features** before utilize the PCA Algorithm, such as

  - Normalize the mean to be 0
  - Do the feature scaling (like the course 1, week 2, housing example)

- **Not bad approach**: project each point down to the z axis.

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618120738.png"/>

  - for this example ğŸ‘† is not bad, because the variance of the data points in z axis is still large, so we still have a lot of information of original data after the projection.

- **Bad choice of the z axis**: the projected data points are squashed together

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618120945.png"/>

- The **best choice** of the z axis:

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618121154.png"/>

  - In PCA algorithm, this axis is called, **the principal component** (first principle component)
  - which means, if you project all the data points onto it, you get the **largest variance**

> The scikit-learn can help you implement this algorithm.

Here are more details about this PCA algorithm:

- How to project a data point $(2,3)$ to the z axis?

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618122431.png"/>

  - Suppose the **length 1 vector** in z axis is  $\begin{bmatrix} 0.71\\0.71 \end{bmatrix}$ 

  - You take dot product of the data point to the z axis' length 1 vector:
    $$
    \begin{bmatrix} 2\\3 \end{bmatrix} \cdot \begin{bmatrix} 0.71\\0.71 \end{bmatrix} = 3.55
    $$

  - This 3.55 is the projected value of data point $(2,3)$ onto the z axis.

- If you were to pick another "2nd principal component", this axis will always be perpendicular to the 1st axis you picked. 

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618122709.png"/>

  - If there will be the 3rd one, this one is also perpendicular to previous two.

How PCA is different from linear regression?

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618123436.png"/>

- **In linear regression**, there are $(x,y)$ pairs, means there exists label $y$ . Its mission is to predict values in the fitted line as close as possible to the ground truth label $y$. Meaning that it is trying to minimize the distance (the **cost**) along $y$ axis.
- **In PCA algorithm**, you are try to reduce the distance between the unlabeled data points to the z axis, to retain variance as large as possible.
- If you have more than 2 features (which is almost all the case), the difference between two algorithm becomes very large

here is an extreme example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618123649.png"/>

- the line that PCA choose is totally different from the linear regression.

Given projected number $z=3.55$, can we reconstruct original coordinates of the data point?

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618124018.png"/>

- we don't have enough info to get it back precisely
- But we can get it back approximately, which is $\begin {bmatrix} 2.52 \\ 2.52 \end {bmatrix}$
- That's not a bad approximation! This denotes the PCA retains a lot information of this data point during projection.

## 7.3 PCA in code

>  How you can use the scikit-learn library to implement PCA

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618134031.png"/>

1. **pre-processing**: feature scaling would be important to help PCA find a good choice of axes for you.
2. call the "`fit`" function to obtain new axes. The mean normalization is included in this step
   - as shown before, these axes (principle components) are perpendicular with each other.
3. Examine how good these axes are, by calling `explained_variance_ratio`.
4. Transform (project) the data onto new axes. 

Example:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618134223.png"/>

1. we are to reduce features of `X` to one

2. that's the array on the left

3. The "explained_variance_ratio" equals 0.992, means that the principle component axis we founded can explain the 99.2% of the variance of the original data.

4. finally, call `inverse_transform` to reconstruct the examples into 2D graph again. We can see the result, now they lies into a single line.

   <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618134300.png"/>

By the way, we can also transform the original data, to 2D still:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618135138.png"/>

- The two new axes $z_1$ explains 99.2% of the variance of the original data and $z_2$ explains 0.8%. 
  - together they explains 100%
- so if you firstly apply this "2-axes" PCA fitting to the dataset, you may then discover that the **axis 1** retains most of the information of the original dataset. Then you can decide just to use this axis to represent the data.

More advice in applying PCA:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618135702.png"/>

1. They are useful in visualizing the data, even for now.
2. Because the modern storage space & transmitting techniques, the PCA used for data compression is less than before.
3. Because the modern deep learning (NN structure), the PCA also is less used for boosting the training speed as it does not help much.

# 8 Reinforcement learning introduction

In machine learning, reinforcement learning is one of those ideas that while not very widely applied in commercial applications yet today, is one of the pillars of machine learning. And has lots of exciting research backing it up and improving it every single day. 

## 8.1 What is Reinforcement Learning?

What is reinforcement learning:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618155145.png"/>

- How to write a programme to fly this helicopter? (compared with quad roter drones, radio controlled helicopters are harder to keep balance in the air)

  - reinforcement learning has been used to get helicopters to fly a wide range of stunts or we call them **aerobatic maneuvers**.

    <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618155313.png"/>

So how do you write a program to do this automatically?

- the task is to find a function, which mappes from the state $s$ to the action $a$

- supervised learning is not recommend, and it is not helpful here

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618155757.png"/>

  - because it is very ambiguous that the action to take (label y) corresponding to the each state (training example x). 

- we instead use **reinforcement learning**

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618160750.png"/>

  - the core of the reinforcement learning is to define a **reward function** 

  - it's like training a puppy dog using behaviorism approach.

    <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618160630.png"/>

  - The job for reinforcement learning is to get more good behaviors for a helicopter and less bad behaviours.

  - in reinforcement learning, you just tell it "what to do" rather than "how to do it". 

    > Let it learn the "secrets & inner techniques" by itself.

  - specifying the **reward function** rather than the optimal action gives you a lot more flexibility in how you design the system.

  - for example, it the helicopter flys well, you give it "+1" reward every second. But if it crashes, you give it "-1000" reward.

Today, reinforcement learning has been successfully applied to a variety of applications:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618161008.png"/>

- **Robot controlling**: in fact later this week in the practice lab, you implement for yourself a reinforcement learning algorithm to land a lunar lander in simulation.

the key idea of reinforcement learning is:

- rather than you needing to tell the algorithm what is the right output y for every single input, 
- all you have to do instead is specify a reward function that tells it when it's doing well and when it's doing poorly.

## 8.2 Mars rover example

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240618162616.png"/>

- The mars rover has 6 states, and not it starts at **state 4**

- The state 1 is the scenario that it encounters some highly scientific valuable surfaces on the Mars. The state 6 is valuable too, but not as much as state 1

  - the reward at state 1 is 100
  - the reward at state 6 is 40
  - Others are 0, because there are not that much interesting science works to be done in them

- The state 1 and 6 are "terminal state" because once get there, the mars rover ends its work for a day.

  - it can either choose to go left and go right.

- At every timestep, the robot is in some states, get to choose some actions, and it also enjoys some rewards $R(s)$. And it also gets to some new states $s^`$ (s prime)
  $$
  (s,a,R(s),s^`)
  $$

- That's the formalism of how a reinforcement learning application works

## 8.3 The Return in the reinforcement learning

But how do you know if a particular set of **rewards** is better or worse than a different set of rewards? The "**Return**" in reinforcement learning allows you to capture that.

- The concept of a return captures that **rewards you can get quicker** are maybe more attractive than rewards that take you a long time to get to

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619151029.png"/>

- In this mars rover example, if you go left, you finaly get a 100 reward. But during each of the timestep, you must multiply the **discount factor** $\gamma$ with the award corresponding to that status

  - here, the discount factor is (0.9) and $0.9^2, 0.9^3$, according to how far the mars rover already gone
  - So the return of go left becomes **72.9**
  - Remember **the first reward is not discounted**

- The return is the reward multiply the discount factor in each step, and summarize them. 

- This gives the algorithm the ability to evaluate between different directions to go (based on the combination of distance and reward) , and pick one with the highest reward.

- If $\gamma = 0.5$, the return to go left is close to the go right, even go left will end up being the status with 100 reward.

  12.5 (left), compared with 10 (right)

In financial applications, the discount factor can be very naturally interpreted as "interest rate" or "time value of money".

Some more examples on "Return":

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619152006.png"/>

- The return depends on the action you take and the start point
  - if you start at status 4, then choose to go left, then you end up with 12.5 return
  - if you start at status 2, then the return is 50
- If you are to go right, things are different.
- Finally, after learning all the returns for every direction from every starting point, you decide that go right when start at **status 5**, instead for **status 1-4**, go left.

Next:

> To formalize the goal of reinforcement learning

## 8.4 Making decisions: Policies in reinforcement learning

There are many different strategies that the algorithm can choose:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619175051.png"/>

- like:
  1. always go for the nearer reward (leftmost / rightmost)
  2. Go for higher reward
  3. go for smaller reward
  4. choose to go left unless one step away from the lesser reward

- we need a function $\pi$ that mappes the **action** $a$ to the current **status** $s$
  - $\pi(5) = left$ means choose to go left when status is 5

To goal for the reinforcement learning:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619175250.png"/>

- call $\pi$ as "policy" is a converntional way, though it is not that natural
- Maybe understand the "policy" as "controller" is better

## 8.5 Review of key concepts

Let's do a quick review of the key concepts and also see how this set of concepts can be used for other applications as well:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619175948.png"/>

- In helicopter example:
  - **states** can be a set of positions of helicopter
  - **actions** is the set of possible ways to move the controls stick
  - the **policy** is to find the proper movement $a$ regarding state $s$ 
- In chess examples:
  - **states** is the positions of all the pieces on the board
  - **reward** is +1 if wins a game, 0 if ties a game
  - the goal is given a board position, to find a good action using **policy** $\pi$

This formalism of a reinforcement learning application actually has a name, **MDP**:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240619180258.png"/>

- ğŸ‘† this is how this process works
- "**The future only depends on the current state**"

Next:

> we'll start to develop an algorithm for picking good actions. 
>
> The first step toward that will be to define and then eventually learn to compute the state action value function. 
>
> This turns out to be one of the key quantities for when we want to develop a learning algorithm.

# 9 State-action value function

## 9.1 State-action value function defination

State-action value function is a key quantity that reinforcement learning algorithm will try to compute.

- This function is typically denoted by the $Q$ 

- This is the main explanation  of this function:

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620101825.png"/>

  - Later, we will find a way to compute the state-action value function

Let's bring the mars rover example back. Here is the optimal policy that the rover moves:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620102043.png"/>

Let's figure out the $Q$ for each state:

- $Q(2, right) = 0+(0.5)\times 0 + (0.5)^2 \times 0 + (0.5)^3 \times 100 = 12.5$
  - Start from state 2 and go right, the trail is: 2 >>> 3 >>> 2 >>> 1
  - note that even go right from state 2 is not a good choice, $Q$ does not judge it right or wrong. But just compute the $Q$ here

- $Q(2, left) = 0+0.5\times 100 = 50$

- $Q(4, left) = 0+(0.5)\times 0 + (0.5)^2 \times 0 + (0.5)^3 \times 100 = 12.5$

- This is the Q-function for each state of this example:

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620102744.png"/>

The best possible return / action from the state $s$ is the largest value of $Q(s,a)$ ğŸ‘‡

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620103408.png"/>

- For every state and for every action when you're in some state $s$, all you have to do is look at the different actions $a$

- You should pick the action $a$ That maximizes $Q(s,a)$, and so $\pi(s)$. 

- This turned to be the optimal action (maximizes the return at this circumstance)

- Sometimes at Internet, someone refer to "$Q^*$" and call this optimal $Q$ function. This is exactly same as the state-action value function in this class

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620103834.png"/>

Next:

> Let's look at some examples that tells us what these values look like

## 9.2 State-action value function example

In order to keep holding our intuition about reinforcement learning problems and how the values of $Q(s,a)$ change depending on the problem will provided an optional lab. That lets you play around modify the mars rover example and see for yourself how $Q(s,a)$ will change.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620104419.png"/>

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620104458.png"/>

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620104557.png"/>

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620104628.png"/>

Next:

> After you play to the lab, we then be ready to come back and talk about what's probably the single most important equation in reinforcement learning, which is something called the **bellman equation**

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620104935.png"/>

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620105017.png"/>

## 9.3 Bellman Equation

So the question is, how do you compute $Q(s,a)$? The Bellman Equation.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620110501.png"/>

- $s,a$ is the start state and action, $s^\prime, a^\prime$ is another state and action

- $$
  Q(s,a) = R(s) + \gamma \max_{a^\prime} Q(s^\prime, a^\prime)
  $$

- å‘å³ï¼š0 + (0.25)*0 + (0.25^2)*0 + (0.25^3)*40 = 0.625
- å‘å·¦ï¼š0 + (0.25)0 + (0.25^2)0 + (0.25^3)0 + (0.25^4)100 = 0.391

To varify this equation:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620110749.png"/>

And to breakdown this equation:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620111422.png"/>

- The bellman equation is cosisted of **two parts**:
  - the reward you get right away (multiplied by $\gamma^0$, which is 1), and
  - the best return from behaving optimally starting from $s^{\prime}$
- **It is the return if you start from state $s$, take action $a$ (once), then behave optimally after that.** 
- We can double check this breakdown in the example we calculated before:
  - <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620111713.png"/>

## 9.4 Random (stocastic) environment

In practice, many robots don't always manage to do exactly what you tell them because of wind blowing and off course and the wheel slipping or something else.

There's a generalization of the reinforcement learning framework we've talked about so far, which **models random or stochastic environments**. 

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620112132.png"/>

- what if 10% of the time, the mars rover actually ends up accidentally slipping and going in the opposite direction? 
  - 0.9 chance to go for the right direction
  - 0.1 chance to go wrong

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620112409.png"/>

So here, we use "**Expected Return**" to denote an average from (maybe *many thousands of*) different returns of movement.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240620112646.png"/>

- So the job of reinforcement learning algorithm is choose a policy $\pi$ in which action $a$ maximizes the expected return ğŸ‘‡

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621091904.png"/>

- now, the Bellman queation consisits of two parts:

  1. the reward you get right away (also called immediate reward), and
  2. what you **expect** to get **on average** of the **future returns**.

Now, back into the optional lab, set the misstep probability to **0.9**:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621134352.png"/>

- You can see the Q-values (optimal returns) have gone done a bit. Because you cannot control the robot as well as before.

# 10 Countinuous state spaces

## 10.1 Examples of continuous state space applications

Robots can be in continuous positions (cannot breakdown into concrete numbers)

- Such as the mars rover can be anywhere on a line

- for another example, when you want to control a toy truck, you have to clearify the **x-position**, the **y-position**, the **speed in x-y-position** and the **orientation** (where it faces) with the **angular velocity** of it.

  <img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621191020.png"/>

  - so the state is an vector of six numbers

Another example: controling the helicopter

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621191544.png"/>

- **Capturing the position of the helicopter**: the x-y-z axis
  - $[x,y,z]$
- **Capturing the gesture of the helicopter**: the roll, the pitch, the yaw
  - $[\phi, \theta, \omega]$
- **Capturing the speed of the helicopter**: the speed for x-y-z and angular velocity of its gesture
  - $[\dot x, \dot y,\dot z, \dot \phi, \dot \theta, \dot \omega]$
- The policy is to look at these 12 numbers and to decide what direction to go.

## 10.2 Lunar lander

The lunar lander lets you land a simulated vehicle on the moon. It's like a fun little video game that's been used by a lot of reinforcement learning researchers.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621193057.png"/>

- You have to choose action(s) to ensure the successful landing

- The state is consists of these numbers:
  $$
  s=\begin{bmatrix} x\\y\\\dot x\\\dot y\\\theta\\\dot theta \\l \\r \end{bmatrix}
  $$

  - **x, y** refers to the position of lunar lander in the 2-D coordinate system
    - $\dot x, \dot y$ refers to the speed in both axes
  - $\theta$ refers to the angle of the lunar lander
  - $l,r$ are two binary numbers, which corresponds to whether the left / right leg is touching the ground

The reward function is as follow:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621193839.png"/>

- You find when you're building your own reinforcement learning application usually takes some thought to specify exactly what you want or don't want and to codify that in the reward function.
- But specify the reward function should still turn out to be much easier to specify the exact right action to take from every single state.
  - this approach (specify the reward function rather than pointing out the exact right direction) is similar to the thought of "cost function" rather than find the minimum value mathematically.
  - "taking little step at a time" rather than "straightforwardly go for the global minimum"

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240621193947.png"/>

- The gamma can be 0.985
- And if you can learn a policy pi that does this then you successfully land this lunar lander 

we're now finally ready to develop a learning algorithm which will turn out to use **deep learning (neural networks)** to come up with a policy to land the lunar lander.

## 10.3 Learning the state-value function

The heart of the learning algorithm is we're going to train a neural network that inputs the current state and the current action, and computes or approximates $Q(s,a)$

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622093134.png"/>

- The input $\vec x$ : 12 number-vector
  - **8 numbers**: to decribe the state
  - **4 numbers** (binary for each): to decribe the action
- The job for this neural network is to output the $Q(s,a)$
  - also, target value $y$

What is different from supervised learning:

- Instead of input a state $s$, then try to output an action $a$,
- **The reinforcement learning algorithm is to input a state-action pair (the 12-number vector), and have it try to output the state-action value function $Q(s,a)$**

- So, whenever your lunar lander is in a state $s$, you can use the trained neural network to compute $Q(s,a)$ for all four actions -- then finally **you just pick whichever values higest**

how do you train a neural network to output $Q(s,a)$?

- to use Bellman's equations to create a training set with lots of examples, x and y.
- and then we'll use supervised learning, to learn a mapping from $x$ to $y$.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622095157.png"/>

- Firstly, we must clarify what are the training examples and targe values in this neural network:
  - **Training examples**: the $s$ and $a$
  - **Target values**: the $R(s) + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime})$

- How can we generate the training examples and labels:

  - we randomly trying out different actions (maybe good or bad) in the lunar lander simulator.

  - And record the state $s^{(1)}$, the action $a^{(1)}$, the reward $R(s^{(1)})$ and the next state $s^{\prime (1)}$ 

  - Maybe you try this over 10,000 times. You get all these datas.

  - And you use $s^{(1)}, a^{(1)}$ to compute each training example $x^{(i)}$; Use $R(s^{(1)}), s^{\prime (1)}$ to compute $y^{(i)}$

- Another thing we need to clearify: initially we don't know the Q-function, so how can we compute $y$?
  - you can start off with taking a totally random guess of it.
  - The algorithm will work nonetheless. But in every step **Q** is just going to be some guess. 
  - They'll get better over time. it turns out of what is the actual Q function

Let's see what is the full learning algrithm:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622095943.png"/>

- The $Q(s^{\prime}, a^{\prime})$ is randomly intialized by this neural network.
- we train the neural network to come up with $Q_{new}$, and this $Q_{new}$ can approximately  mapping $x$ to $y$ .
- Finally you update Q to the $Q_{new}$. That's one training loop. If you train this NN for more times, and update $Q$ each time, then the result would be better and better.

The algorithm you just saw is sometimes called the **DQN algorithm** which stands for Deep Q-Network. Because you're using deep learning and neural network to train a model to learn the Q functions.

Next:

>  if you use the algorithm we just described, it will just sort of work. But it turns out that with a couple of refinements to the algorithm, it can work much better.

## 10.4 Algorithm refinement: Improved neural network structure

This is the nerual network archetecture we saw previously:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622110347.png"/>

- This is inefficient because we have to carry out inference **4 times** from every single state.
  - because there are 4 different actions can take.

So, the modified NN architecture looks like this:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622110715.png"/>

- Output four $Q(s,a)$ simultaneously. 
- This utilizes the most of the modern GPU computation
- now, given a state $s$, we can run inference just once, and get all four values, to pick an action $a$ that maximizes $Q(s,a)$ 
- this also boosts the computation of $\max_{a^{\prime}} Q(s^{\prime}, a^{\prime})$

We will use this architecture in the practice lab.

Next:

> there's one other idea that'll help the algorithm a lot which is something called an Epsilon-greedy policy, which affects how you choose actions even while you're still learning. 

## 10.5 Algorithm refinement: $\epsilon$-greedy policy

The learning algorithm that we developed, even while you're still learning how to approximate $Q(s,a)$, you need to take some actions in the lunar lander. How do you pick those actions while you're still learning?

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622111227.png"/>

- the most common way: $\epsilon$-greedy policy

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622112630.png"/>

- Option 1 is good, but not the best
- Option 2 is what we typically use in practice

Why option 2 is better:

- sometimes because of the random initialization, the $Q(s,main)$ is always low, means the return of firing the main engine is always low.
- In this situation, **if we use option 1**, then the algorithm will never ever try to fire the main engine, thus will never learn it. 
  - So that the algorithm will never have a chance to figure out that "firing the main engine sometimes can be a good idea!"
- **In Option 2**, this allows the neural network can learn to overcome its own possible preconceptions about what might be a bad idea that turns out not to be the case.

**Option 2** has the name: $\epsilon$-greedy policy

- In this example, the 0.95 is called "Greedy" or "Exploitation" 

  > let's exploit everything we've learned to do the best we can.

- the $\epsilon$ specifies the possibility (0.05) in which we explore a little bit of what we haven't done before.

The common trick is to start off $\epsilon$ high (initially take almost random actions) and gradually decrease it.

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622112652.png"/>

- So that over time, you will start to less act randomly and using the experience you learned to act.

What's more:

1. After applying the more efficient NN architecture and $\epsilon$-greedy policy, the algorithm works pretty well on the lunar lander.
2. Reinforcement learning algorithms are much more finicky to little choices of parameters, compared with supervised learning

Next:

> to drive a couple more algorithm refinements, mini batching, and also using soft updates

## 10.6 Algorithm refinement: Mini-batch and soft update

The first idea is called using mini-batches, and this turns out to be an idea they can both speedup your reinforcement learning algorithm and it's also applicable to supervised learning.

The mini-batches:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622121541.png"/>

- this is the exact housing price example in course 1 (supervised learning)
- Sometimes, the training set size $m$ can be 100,000,000, in such circumstance, the each computation step of gradient descent will be too slow. (because will need to scan over all the 100,000,000 examples time after time)
- **The mini-batch**: take 1000 examples as $m^\prime$ 

For each iteration, the algorithm only look at one mini-batch. 

- For example, one way is in iteration 1, look at mini-batch 1
- in iteration 2, look at mini-batch 2
- ....

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622121900.png"/>

The difference in loss:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622122302.png"/>

- On average, the mini-batch gradient descent will tend to go onto the global minimum. 
- Even its trail looks like a little confused (because the ramdomly choosed mini-batches), it is totally bearable compared with the benefits that the computational resources it saved and the accelaration it brings about

This brings you a much faster learning algorithm especially when you have a very large training set.

In reinforcement learning, you might choose only a subset of the total 10,000 training examples (use only **1,000** at a time rather than 10,000).

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622122639.png"/>

- This will overall speed up the learning algorithm.

And, **soft updates** can make the algorithm coverge more reliably:

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622123213.png"/>

- pay attention to the **"set $Q=Q_{new}$"** step.

- To prevent $W_{new}, B_{new}$ is worse than previous $W, B$

- In each step of updating the $W,B$, we set:
  $$
  W=0.01\times W_{new} + 0.99\times W \\
  B=0.01\times B_{new} + 0.99\times B
  $$

- This is called "soft update" - make more gradual changes.

## 10.7 The state of the reinforcement learning

> what I hope to do is share with you a practical sense of where reinforcement learning is today in terms of its utility for applications

<img src="https://siyuan-harry.oss-cn-beijing.aliyuncs.com/oss://siyuan-harry/20240622123729.png"/>

- Succeed in simulation is not a guarantee on the succeed in the real world.
- The potential for reinforcement learning for future applications is very large

## Practice lab

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ—¶ï¼Œåˆ›å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œï¼ˆQ-Networkå’ŒTarget Q-Networkï¼‰çš„ä¸»è¦åŸå› æ˜¯ä¸ºäº†æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚è¿™ä¸¤ä¸ªç½‘ç»œçš„ä½œç”¨å’ŒåŒºåˆ«å¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç‚¹æ¥è§£é‡Šï¼š

1. **ç›®æ ‡å€¼çš„è®¡ç®—ï¼š**
   åœ¨Q-Learningä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ç›®æ ‡å€¼ï¼ˆtarget valueï¼‰ï¼Œå³ï¼š
   \[
   y = R + \gamma \max_{a'}Q(s',a';w)
   \]
   è¿™é‡Œï¼Œ\(R\) æ˜¯å³æ—¶å¥–åŠ±ï¼Œ\(\gamma\) æ˜¯æŠ˜æ‰£å› å­ï¼Œ\(Q(s',a';w)\) æ˜¯ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œä»·å€¼å‡½æ•°ã€‚

2. **ç›®æ ‡å€¼çš„å˜åŒ–ï¼š**
   å¦‚æœç›´æ¥ä½¿ç”¨Q-Networkæ¥è®¡ç®—ç›®æ ‡å€¼ï¼Œé‚£ä¹ˆç›®æ ‡å€¼ \(y\) ä¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­ä¸æ–­å˜åŒ–ï¼Œå› ä¸ºQ-Networkçš„æƒé‡ \(w\) åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šæ›´æ–°ã€‚è¿™ç§ä¸æ–­å˜åŒ–çš„ç›®æ ‡å€¼ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šå’Œéœ‡è¡ï¼Œéš¾ä»¥æ”¶æ•›ã€‚

3. **å¼•å…¥Target Q-Networkï¼š**
   ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Target Q-Networkï¼ˆç›®æ ‡Qç½‘ç»œï¼‰ï¼Œå…¶ç»“æ„ä¸Q-Networkç›¸åŒï¼Œä½†æƒé‡ \(w^-\) æ›´æ–°å¾—æ›´æ…¢ã€‚ä½¿ç”¨Target Q-Networkè®¡ç®—ç›®æ ‡å€¼ï¼š
   \[
   y = R + \gamma \max_{a'}\hat{Q}(s',a';w^-)
   \]
   è¿™é‡Œï¼Œ\(\hat{Q}\) æ˜¯Target Q-Networkï¼Œ\(w^-\) æ˜¯å…¶æƒé‡ã€‚

4. **æƒé‡æ›´æ–°ï¼š**
   æ¯éš”å›ºå®šçš„æ—¶é—´æ­¥ï¼ˆé€šå¸¸æ˜¯Cæ­¥ï¼‰ï¼Œæˆ‘ä»¬å°†Q-Networkçš„æƒé‡ \(w\) å¤åˆ¶åˆ°Target Q-Networkçš„æƒé‡ \(w^-\) ä¸­ã€‚ä¸ºäº†ä½¿ç›®æ ‡å€¼å˜åŒ–å¾—æ›´åŠ å¹³æ»‘ï¼Œæˆ‘ä»¬ä½¿ç”¨è½¯æ›´æ–°ï¼ˆsoft updateï¼‰ï¼š
   \[
   w^- \leftarrow \tau w + (1 - \tau) w^-
   \]
   å…¶ä¸­ï¼Œ\(\tau \ll 1\)ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†Target Q-Networkçš„æƒé‡æ›´æ–°å¾—éå¸¸ç¼“æ…¢ï¼Œä»è€Œä½¿ç›®æ ‡å€¼ \(y\) çš„å˜åŒ–æ›´åŠ å¹³æ»‘å’Œç¨³å®šã€‚

5. **ç¨³å®šæ€§æå‡ï¼š**
   é€šè¿‡å¼•å…¥Target Q-Networkï¼Œæˆ‘ä»¬å°†ç›®æ ‡å€¼çš„å˜åŒ–é€Ÿåº¦æ§åˆ¶åœ¨ä¸€ä¸ªè¾ƒä½çš„æ°´å¹³ï¼Œé¿å…äº†ç›®æ ‡å€¼é¢‘ç¹å˜åŒ–å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚è¿™ç§è®¾è®¡å¤§å¤§æé«˜äº†DQNç®—æ³•çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚

æ€»ç»“èµ·æ¥ï¼Œåˆ›å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œï¼ˆQ-Networkå’ŒTarget Q-Networkï¼‰çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…ç›®æ ‡å€¼é¢‘ç¹å˜åŒ–å¯¼è‡´çš„ä¸ç¨³å®šå’Œéœ‡è¡ï¼Œä»è€Œæé«˜ç®—æ³•çš„æ”¶æ•›æ€§å’Œæ•ˆæœã€‚

æˆ‘ï¼šé‚£ä¹ˆï¼Œåœ¨ä¼ ç»Ÿçš„supervised learningä¸­ï¼Œä¾‹å¦‚ç”¨linear regression æ¥é¢„æµ‹æˆ¿ä»·çš„ç»å…¸æ¡ˆä¾‹ä¸­ï¼Œä¸ºä»€ä¹ˆç›®æ ‡yå€¼å°±ä¸ä¼šå˜åŒ–ï¼Ÿæ˜¯å› ä¸ºå®ƒæ˜¯ç»™å®šçš„training seté‡Œé¢çš„ï¼Œå¯¹å—ï¼Ÿ

è€Œåœ¨è¿™ä¸ªDQNä¸­ï¼Œyå€¼ä¸æ˜¯ç»™å®šçš„å›ºå®šå€¼ï¼Œè€Œæ˜¯ç”±ç½‘ç»œåŸºäºæƒé‡æ¥generateçš„ ï¼ˆç±»ä¼¼äºæ˜¯ç½‘ç»œè‡ªå·±å­¦ä¹ è‡ªèº«ï¼Œå’Œè‡ªå·±å¯¹å¼ˆçš„çŠ¶æ€ï¼‰ï¼Œæ‰€ä»¥å¿…é¡»è¦é€šè¿‡å¢åŠ ä¸€ä¸ªQç½‘ç»œï¼Œé¿å…å•ä¸€çš„Qç½‘ç»œè‡ªå·±å‡ºç°â€œå·¦å³äº’æâ€ä¹Ÿå°±æ˜¯å¯¼è‡´æ— æ³•å­¦ä¹ ã€æ— æ³•è¿›æ­¥çš„æƒ…å†µã€‚æˆ‘çš„ç†è§£æ­£ç¡®å—ï¼Ÿ

GPTï¼šä½ çš„ç†è§£åŸºæœ¬æ˜¯æ­£ç¡®çš„ã€‚è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¯”è¾ƒä¸€ä¸‹ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å’ŒDQNä¸­çš„ç›®æ ‡å€¼é—®é¢˜ã€‚

### ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ 

åœ¨ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼ˆå¦‚çº¿æ€§å›å½’ï¼‰ä¸­ï¼š

- **ç›®æ ‡å€¼ï¼ˆyï¼‰æ˜¯å›ºå®šçš„ï¼š** è®­ç»ƒæ•°æ®é›†ä¸­çš„ç›®æ ‡å€¼ \( y \) æ˜¯é¢„å…ˆç»™å®šçš„ï¼Œä¸ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ¿ä»·é¢„æµ‹çš„æ¡ˆä¾‹ä¸­ï¼Œç›®æ ‡å€¼å°±æ˜¯å·²çŸ¥çš„æˆ¿ä»·æ•°æ®ã€‚
- **å›ºå®šç›®æ ‡å€¼å¸¦æ¥çš„ç¨³å®šæ€§ï¼š** ç”±äºç›®æ ‡å€¼æ˜¯å›ºå®šçš„ï¼Œæ¨¡å‹åªéœ€è¦ä¸æ–­è°ƒæ•´å…¶å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ˆå¦‚å‡æ–¹è¯¯å·®ï¼‰ã€‚è¿™ç§å›ºå®šçš„ç›®æ ‡å€¼ä½¿å¾—è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹ç¨³å®šï¼Œæ¨¡å‹å¯ä»¥é€æ¸æ”¶æ•›åˆ°ä¸€ä¸ªè¾ƒä¼˜çš„è§£ã€‚

### æ·±åº¦Qå­¦ä¹ ï¼ˆDQNï¼‰

åœ¨DQNä¸­æƒ…å†µæœ‰æ‰€ä¸åŒï¼š

- **ç›®æ ‡å€¼ï¼ˆyï¼‰æ˜¯åŠ¨æ€çš„ï¼š** ç›®æ ‡å€¼ \( y \) æ˜¯åŸºäºå½“å‰Qç½‘ç»œçš„è¾“å‡ºè®¡ç®—çš„ï¼Œè€ŒQç½‘ç»œçš„æƒé‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šæ›´æ–°ã€‚è¿™æ„å‘³ç€ç›®æ ‡å€¼æœ¬èº«æ˜¯åŠ¨æ€å˜åŒ–çš„ã€‚
- **åŠ¨æ€ç›®æ ‡å€¼å¸¦æ¥çš„ä¸ç¨³å®šæ€§ï¼š** ç”±äºç›®æ ‡å€¼åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½å¯èƒ½å˜åŒ–ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šå’Œéœ‡è¡ã€‚æ¨¡å‹åœ¨è°ƒæ•´å‚æ•°æ—¶ï¼Œç›®æ ‡å€¼ä¹Ÿåœ¨å˜åŒ–ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹éš¾ä»¥æ‰¾åˆ°ä¸€ä¸ªç¨³å®šçš„ä¼˜åŒ–æ–¹å‘ã€‚

### å¼•å…¥Target Q-Networkçš„åŸå› 

ä¸ºäº†åº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥äº†Target Q-Networkï¼š

- **ç¨³å®šç›®æ ‡å€¼ï¼š** Target Q-Networkçš„æƒé‡æ›´æ–°å¾—æ›´æ…¢ï¼Œä½¿å¾—ç›®æ ‡å€¼ \( y \) çš„å˜åŒ–æ›´åŠ å¹³ç¨³ï¼Œä»è€Œé¿å…äº†é¢‘ç¹å˜åŒ–çš„ç›®æ ‡å€¼å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚
- **é¿å…â€œå·¦å³äº’æâ€ï¼š** å°±åƒä½ æåˆ°çš„ï¼Œè¿™ç§è®¾è®¡é¿å…äº†å•ä¸€Qç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªæˆ‘å¯¹æŠ—çš„æƒ…å†µã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªæ›´æ–°è¾ƒæ…¢çš„Target Q-Networkï¼Œå¯ä»¥ä½¿ç›®æ ‡å€¼çš„å˜åŒ–é€Ÿåº¦å‡ç¼“ï¼Œä»è€Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šã€‚

### æ€»ç»“

- **ç›‘ç£å­¦ä¹ ï¼š** ç›®æ ‡å€¼ \( y \) æ˜¯é¢„å…ˆç»™å®šçš„å›ºå®šå€¼ï¼Œè®­ç»ƒè¿‡ç¨‹ç¨³å®šã€‚
- **DQNï¼š** ç›®æ ‡å€¼ \( y \) æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œéœ€è¦å¼•å…¥Target Q-Networkæ¥ç¨³å®šç›®æ ‡å€¼ï¼Œé¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚

ä½ çš„ç†è§£æ˜¯æ­£ç¡®çš„ï¼Œé€šè¿‡å¢åŠ ä¸€ä¸ªTarget Q-Networkï¼ŒDQNå¯ä»¥é¿å…å•ä¸€Qç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªæˆ‘å¯¹æŠ—å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œä»è€Œä½¿å­¦ä¹ è¿‡ç¨‹æ›´åŠ ç¨³å®šå’Œæœ‰æ•ˆã€‚

ä½ çš„é—®é¢˜æ˜¯éå¸¸å…³é”®ä¸”å¸¸è§çš„ä¸€ä¸ªå›°æƒ‘ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹å…¶ä¸­çš„ç»†èŠ‚ï¼Œä»¥æ¾„æ¸…ä½ çš„ç–‘é—®ã€‚

### åˆå§‹åŒ–æ—¶çš„æƒé‡

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œç¡®å®ä¼šåˆå§‹åŒ–ä¸¤ä¸ªç½‘ç»œï¼ˆQ-Network å’Œ Target Q-Networkï¼‰ï¼Œå¹¶ä¸”å®ƒä»¬çš„æƒé‡æ˜¯ç›¸åŒçš„ã€‚ä½†æ˜¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬çš„æƒé‡æ›´æ–°æ–¹å¼å’Œé¢‘ç‡æ˜¯ä¸åŒçš„ï¼Œè¿™å¯¼è‡´äº†å®ƒä»¬çš„è¾“å‡ºä¼šæœ‰æ‰€ä¸åŒã€‚

### æƒé‡æ›´æ–°æœºåˆ¶

1. **Q-Networkçš„æƒé‡æ›´æ–°ï¼š**
   - Q-Networkçš„æƒé‡ \( w \) åœ¨æ¯ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­éƒ½ä¼šé€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œæ›´æ–°ã€‚è¿™æ„å‘³ç€åœ¨æ¯ä¸€ä¸ªmini-batchè®­ç»ƒä¹‹åï¼ŒQ-Networkçš„æƒé‡éƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚

2. **Target Q-Networkçš„æƒé‡æ›´æ–°ï¼š**
   - Target Q-Networkçš„æƒé‡ \( w^- \) å¹¶ä¸æ˜¯åœ¨æ¯ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°çš„ã€‚é€šå¸¸æ˜¯æ¯éš”å›ºå®šçš„æ—¶é—´æ­¥ï¼ˆä¾‹å¦‚ï¼Œæ¯1000æ­¥ï¼‰æ‰ä¼šå°†Q-Networkçš„æƒé‡å¤åˆ¶åˆ°Target Q-Networkä¸­ã€‚
   - è¿™ç§è¾ƒæ…¢çš„æ›´æ–°é¢‘ç‡ä½¿å¾—Target Q-Networkçš„æƒé‡ç›¸å¯¹ç¨³å®šï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªç¨³å®šçš„ç›®æ ‡å€¼ \( y_i \)ã€‚

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„å·®å¼‚

ç”±äºQ-Networkçš„æƒé‡ \( w \) æ˜¯åœ¨æ¯ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°çš„ï¼Œè€ŒTarget Q-Networkçš„æƒé‡ \( w^- \) æ˜¯åœ¨å›ºå®šçš„æ—¶é—´æ­¥ä¹‹åæ‰æ›´æ–°ï¼Œè¿™å°±å¯¼è‡´äº†åœ¨å¤§å¤šæ•°æ—¶é—´é‡Œï¼Œè¿™ä¸¤ä¸ªç½‘ç»œçš„æƒé‡æ˜¯ä¸åŒçš„ã€‚

### è®¡ç®—ç›®æ ‡å€¼å’ŒæŸå¤±

åœ¨ä¸€ä¸ªmini-batchä¸­ï¼š

1. **è®¡ç®—ç›®æ ‡å€¼ \( y_i \)ï¼š**
   - ä½¿ç”¨Target Q-Networkè®¡ç®—ç›®æ ‡å€¼ \( y_i \)ï¼š
     \[
     y_i = r_i + \gamma \max_{a'} \hat{Q}(s'_i, a'; w^-)
     \]

2. **è®¡ç®—å½“å‰Qå€¼ \( Q(s_i, a_i; w) \)ï¼š**
   - ä½¿ç”¨å½“å‰çš„Q-Networkè®¡ç®—æ¯ä¸ªç»éªŒçš„Qå€¼ \( Q(s_i, a_i; w) \)ã€‚

3. **è®¡ç®—æŸå¤±å‡½æ•°ï¼š**
   - ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è®¡ç®—æŸå¤±ï¼š
     \[
     \text{Loss} = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - Q(s_i, a_i; w) \right)^2
     \]

### ä¸ºä»€ä¹ˆMSEä¸ä¸º0

ç”±äºQ-Networkçš„æƒé‡åœ¨æ¯ä¸€æ­¥éƒ½åœ¨æ›´æ–°ï¼Œè€ŒTarget Q-Networkçš„æƒé‡åªæœ‰åœ¨å›ºå®šçš„æ—¶é—´æ­¥ä¹‹åæ‰æ›´æ–°ï¼Œè¿™å°±å¯¼è‡´äº†å®ƒä»¬çš„è¾“å‡ºä¸ä¼šå®Œå…¨ç›¸åŒã€‚å› æ­¤ï¼Œç›®æ ‡å€¼ \( y_i \) å’Œå½“å‰Qå€¼ \( Q(s_i, a_i; w) \) ä¹‹é—´ä¼šæœ‰å·®å¼‚ï¼Œå¯¼è‡´æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰ä¸ä¸º0ã€‚

### æ€»ç»“

- **åˆå§‹åŒ–æ—¶æƒé‡ç›¸åŒï¼š** åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼ŒQ-Networkå’ŒTarget Q-Networkçš„æƒé‡æ˜¯ç›¸åŒçš„ã€‚
- **è®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡ä¸åŒï¼š** Q-Networkçš„æƒé‡åœ¨æ¯ä¸€æ­¥éƒ½æ›´æ–°ï¼Œè€ŒTarget Q-Networkçš„æƒé‡æ›´æ–°é¢‘ç‡è¾ƒä½ã€‚
- **è¾“å‡ºå€¼ä¸åŒï¼š** ç”±äºæƒé‡æ›´æ–°é¢‘ç‡ä¸åŒï¼ŒQ-Networkå’ŒTarget Q-Networkçš„è¾“å‡ºå€¼ä¸åŒï¼Œå¯¼è‡´ç›®æ ‡å€¼ \( y_i \) å’Œå½“å‰Qå€¼ \( Q(s_i, a_i; w) \) ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚
- **MSEä¸ä¸º0ï¼š** ç”±äºä¸Šè¿°å·®å¼‚ï¼ŒæŸå¤±å‡½æ•°ï¼ˆMSEï¼‰ä¸ä¼šä¸º0ã€‚

å¸Œæœ›è¿™èƒ½æ¾„æ¸…ä½ çš„ç–‘é—®ã€‚å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶æé—®ï¼

### ä½†æ˜¯

é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæ˜æ˜ä¸¤ä¸ªç½‘ç»œçš„æƒé‡æ˜¯åŒæ­¥æ›´æ–°çš„ï¼Œå´ä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆçš„MSEè®¡ç®—å‘¢ï¼Ÿï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼ŒMSEä¸ä¸ºé›¶ï¼‰

GPT: ç»™å‡ºäº†è¿™ä¸ª

```python
# è®¡ç®—ç›®æ ‡å€¼
max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)
y_targets = rewards + gamma * max_qsa * (1-done_vals)

# è®¡ç®—å½“å‰Qå€¼
q_values = q_network(states)
q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),
                                            tf.cast(actions, tf.int32)], axis=1))

# è®¡ç®—MSE
loss = MSE(y_targets, q_values)

```

æˆ‘ï¼šå“¦ï¼æˆ‘æ‡‚äº†ã€‚ç›®æ ‡å€¼y_iæ˜¯é€šè¿‡ rewards + gamma * max_qsa * (1-done_vals) è¿™ä¸ªå¼å­ç®—å‡ºæ¥çš„ï¼Œè€ŒQå€¼æ˜¯Q-Networkç›´æ¥ç®—å‡ºæ¥çš„ï¼è¿™æ˜¯å®ƒä¿©æœ‰å·®åˆ«çš„åŸå› 
